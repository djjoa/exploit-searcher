import requests 
from bs4 import BeautifulSoup
import argparse
from termcolor import colored 
import json
import re 

parser = argparse.ArgumentParser()
parser.add_argument("-v", "--verbose",action="store_true", 
                help="doesn't actually do anything yet")

parser.add_argument("-s", "--search", type=str,
                help="the search term to query all databases for (exploit, vulnerability, CVE, technology, framework)",
                required=True)

parser.add_argument("-e", "--engine", type=str, choices=['edb', 'rapid7', 'packetstorm', 'cve', 'nvd'],
                help="engines to use in query; defaults to ALL")

parser.add_argument("-c", "--cve", type=str,
                help="CVE ID to refine search")

args = parser.parse_args()
SEARCHTERM = args.search
CVE_ID = args.cve
interesting = ['root', 'code execution', 'exploit', 'command','execute','malicious','payload',
                'remote','code','execution','arbitrary','information','leak', 'vulnerability', 'unrestricted', 'remotely']

def banner(title):
    center = str(title).center(20)
    h1 = colored('◄✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸►' , 'blue')
    h2 = colored(f'◄✸✸✸✸✸{center}✸✸✸✸✸►' , 'blue')
    h3 = colored('◄✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸✸►' , 'blue')
    print(f"{h1}\n{h2}\n{h3}")

def resultify(key_string, value_string, color='yellow',end=False): # end = set the end rather than the text color for the key
    if end == True: 
        print(f"{colored(key_string, 'yellow')}: {colored(value_string, color)}")
        return
    print(f"{colored(key_string, color)}: {value_string}")

def query_nvd(): # DEPRECIATED
    ### NVD Section 
    url = f"https://nvd.nist.gov/vuln/search/results?form_type=Basic&results_type=overview&query={SEARCHTERM}&search_type=all&isCpeNameSearch=false"
    nvd_page = requests.get(url)

    soup = BeautifulSoup(nvd_page.content, 'html.parser') 
    # Find a single id 
    # results = soup.find(id="ResultsContainer")
        # prettyfied print(results.prettify())
    
    # get a list containing all tr, th tbody and td elements + more
    cve_results = soup.find('table', class_='table table-striped table-hover')
    
    # extract the chunk of CVE data form the `tbody` element
    cve_tbody = cve_results.find('tbody')
    
    # extract all the CVE links from the `tbody` [x]
    cve_names = cve_tbody.find_all('th')
    
    # create the return url for each CVE [x]
    cve_urls = [f'https://nvd.nist.gov/vuln/detail/{i.text}' for i in cve_names]

    # grab realted scores [v3.1 , v2.0]
    cve_scores = [] 
    double_cve_scores = [score.text for score in cve_tbody.find_all('a', class_='label')] 
    for idx in range(0, len(double_cve_scores), 2): 
        this = double_cve_scores[idx]
        other = double_cve_scores[idx+1]
        cve_scores.append([this,other])
            
    # grab publication dates [x]
    cve_published_dates = []
    for i in range(0,19): 
        publish_date = cve_tbody.find('span', {'data-testid':f"vuln-published-on-{i}"})
        cve_published_dates+=publish_date

    # CVE paragraph descriptions [x]
    cve_summaries = [p.text for p in cve_tbody.find_all('p')]

    final_entries = zip(cve_urls, cve_summaries, cve_scores, cve_published_dates)

    banner('NVD Results')
    for idx, entry in enumerate(final_entries): 
        url, summary, score, date = entry
        scorev3 = score[0]
        scorev2 = score[1]

        if 'CRITICAL' in scorev3 or 'HIGH' in scorev3: 
            scorev3 = colored(scorev3, 'red')
        
        if 'CRITICAL' in scorev2 or 'HIGH' in scorev2: 
            scorev2 = colored(scorev2, 'red')

        if 'CRITICAL' in scorev3 or 'HIGH' in scorev3 and 'CRITICAL' in scorev2 or 'HIGH' in scorev2: 
            summary = colored(summary, 'red')

        
        print(f"{colored('URL','yellow')}: {url}\n{colored('CVE SCORES: V3.1','yellow')}: {scorev3} {colored('V2.0','yellow')}: {scorev2}\n{colored('SUMMARY','yellow')}: {summary}\n{colored('DATE','yellow')}: {date}\n")

def query_packetstorm(): # bugg with idx+1 []TODO 
    banner('Packet Storm')
    
    url = f"https://packetstormsecurity.com/search/?q={SEARCHTERM}"
    page = requests.get(url)
    text = page.text
    base_url = 'https://packetstormsecurity.com'
    # parse number of "Page y of y"
    n_pages = list(set(re.findall(r'href="/search/files/page(\d)/\?q=.*?"',text))) #grab 1,2,3,x for pagination
    n_pages_len = len(n_pages)

    # pagination link examples
    # url = f"https://packetstormsecurity.com/search/page1/?q={SEARCHTERM}"
    # https://packetstormsecurity.com/search/files/page3/?q=shellshock
    all_page_urls = [f"https://packetstormsecurity.com/search/files/page{i}/?q={SEARCHTERM}" for i in range(1,n_pages_len+1)] # works

    
    for link in all_page_urls: #loop around pagination
        # set up BS4 object 
        url = link
        page = requests.get(url)
        soup = BeautifulSoup(page.content, 'html.parser')

        div_frame = soup.find('div', {'id':'m'})
        exploit_frames = div_frame.find_all('dl')#boxes containing exploit name, author, tags, date posted, and advisories
        

        for frame in exploit_frames: 
            #TODO: count = 0 if count is less than number of exploits passed ... 
            exploit_link = frame.find('dt').a['href'] 
            exploit_title = frame.find('dt').a.text 
            exploit_date = frame.find('dd', class_='datetime').a.text
            exploit_description = frame.find('dd', class_='detail').p.text
            exploit_tags = ''.join([str(a.text).replace('tags | ','') for a in frame.find_all('dd', class_='tags')]) #string-list 
            exploit_cves = ''.join([str(dd.text).replace('advisories | ','') for dd in frame.find_all('dd', class_='cve') ]) #string-list 
            exploit_systems = ''.join([str(dd.text).replace('systems | ','') for dd in frame.find_all('dd', class_='os') ]) #string-list 

            # Highlight remote command execution 
            compare_interesting = [i.lower() for i in exploit_title.split(' ')] 
            if  any(item in interesting for item in compare_interesting) : 
                exploit_title = colored(exploit_title, 'red')

            # Highlight root, exploit, RCE in tags 
            split_tags = [t.strip() for t in exploit_tags.split(',')]
            if any(item in interesting for item in split_tags): 
                exploit_tags = colored(exploit_tags, 'red')

            # Colorize certain trigger words on exploit description 
            exploit_description = exploit_description.split(' ')
            colored_description = []
            for word in exploit_description: 
                # if there is a juicy word highlight it red in summary output
                if word in interesting: 
                    colored_description.append(colored(word,'red'))
                else: 
                    colored_description.append(word)
            #reassign the cve description
            exploit_description = ' '.join(colored_description)


            resultify('TITLE', exploit_title)
            resultify('SUMMARY', exploit_description)
            resultify('DATE', exploit_date)
            if exploit_cves: 
                resultify('CVE', exploit_cves)
            if exploit_systems: 
                resultify('SYSTEMS', exploit_systems)
            resultify('TAGS', exploit_tags)     
            resultify('URL', f'{base_url+exploit_link}', 'blue', end=True)
            print() # add space between exploit outputs 
        

    
    
    
    
    # url = f"https://packetstormsecurity.com/search/?q={SEARCHTERM}"
    # packet_storm_page = requests.get(url)
    # soup = BeautifulSoup(packet_storm_page.content, 'html.parser') 
    # #exploit_results = soup.find_all('dl', class_='file')
    
    # titles = [str(a.text) for a in soup.find_all('a', class_='ico text-plain')]
    # ref_links = [f"https://packetstormsecurity.com{a['href']}" for a in soup.find_all('a', class_='ico text-plain')]
    # summaries = [str(dd.text) for dd in soup.find_all('dd', class_='detail') ]
    # cve_ids = [str(dd.text).replace('advisories | ','') for dd in soup.find_all('dd', class_='cve') ]
    # compatable_systems = [str(dd.text).replace('systems | ','') for dd in soup.find_all('dd', class_='os') ]
    # publish_dates = [str(dd.text).replace('Posted ','') for dd in soup.find_all('dd', class_='datetime') ] 
    # tags = [str(dd.text).replace('tags | ','') for dd in soup.find_all('dd', class_='tags') ] 
  
    # banner('Packet Storm')
    # for i in range(0, len(titles)-1): 
    #     #interesting = ['root', 'remote', 'code execution', 'exploit', 'command']
    #     tag = tags[i]
    #     summary = summaries[i]
    #     title = titles[i]

    #     # Highlight remote command execution 
    #     compare_interesting = [i.lower() for i in title.split(' ')] 
    #     if  any(item in interesting for item in compare_interesting) : 
    #         title = colored(title, 'red')

    #     # Highlight root, exploit, RCE in tags 
    #     split_tags = [t.strip() for t in tag.split(',')]
    #     if any(item in interesting for item in split_tags): 
    #         tag = colored(tag, 'red')

    #     resultify('TITLE', title)
    #     resultify('URL', ref_links[i])
    #     resultify('SUMMARY', summary)
    #     resultify('DATE', publish_dates[i])
    #     resultify('CVE', cve_ids[i])
    #     resultify('SYSTEMS', compatable_systems[i])
    #     resultify('TAGS', tag)
        

        # print(f"{colored('TITLE','yellow')}: {title}")
        # print(f"{colored('URL','yellow')}: {ref_links[i]}")
        # print(f"{colored('SUMMARY','yellow')}: {summary}") 
        # print(f"{colored('DATE','yellow')}: {publish_dates[i]}")
        # print(f"{colored('CVE','yellow')}: {cve_ids[i]}")
        # print(f"{colored('SYSTEMS','yellow')}: {compatable_systems[i]}")
        # print(f"{colored('TAGS','yellow')}: {tag}")
        print()         

def nvd_api(term=SEARCHTERM):
     
    # Dev help https://nvd.nist.gov/developers/vulnerabilities
    # other people doing the same thing https://linuxtut.com/en/97e385ed0a78dc28534f/
    ### 11 Results (grabs the most results) 
    banner('NVD Results')
    
    nvd_url = f'https://services.nvd.nist.gov/rest/json/cves/1.0?apiKey=857af2a5-d596-4593-a3a6-afda6cf9ba40&resultsPerPage=100&keyword={term}' 
    cves = requests.get(nvd_url)
    json_cves = json.loads(cves.text)
    

    vulns = json_cves['result']['CVE_Items']
    for vuln in vulns:
        cve_id = vuln['cve']['CVE_data_meta']['ID'] #"ID": "CVE-2021-44224",
        cve_title = vuln['cve']['references']['reference_data'][0]['name'] # "name":"[oss-security] 20211220 CVE-2021-44224: Apache HTTP Server: Possible NULL dereference or SSRF in forward proxy configurations in Apache HTTP Server 2.4.51 and earlier",
        cve_reflink = vuln['cve']['references']['reference_data'][0]['url']
        
        #update these for more colored output in the summary 
        cve_description = vuln['cve']['description']['description_data'][0]['value']
        cve_description_split = cve_description.split(' ')
        colored_description = []
        for word in cve_description_split: 
            # if there is a juicy word highlight it red in summary output
            if word in interesting: 
                colored_description.append(colored(word,'red'))
            else: 
                colored_description.append(word)
        #reassign the cve description
        cve_description = ' '.join(colored_description)
        
        if 'baseMetricV3' in vuln['impact']:
            v3score = vuln['impact']['baseMetricV3']['cvssV3']['baseScore'] # CRIT|HIGH|MED|LOW
            v3severity = vuln['impact']['baseMetricV3']['cvssV3']['baseSeverity'] # CRIT|HIGH|MED|LOW
            if v3severity == 'HIGH' or v3severity == 'CRITICAL': 
                v3severity = colored(v3severity, 'red')
                v3score = colored(v3score, 'red')
        else : 
            v3score = None
            v3severity = None
        
        if 'baseMetricV2' in vuln['impact']:
            v2score = vuln['impact']['baseMetricV2']['cvssV2']['baseScore'] # HIGH|MED|LOW
            v2severity = vuln['impact']['baseMetricV2']['severity'] # HIGH|MED|LOW
            
            if v2severity == 'HIGH': # set both to red 
                v2severity = colored(v2severity, 'red')
                v2score = colored(v2score, 'red')
        else: 
            v2score = None
            v2severity = None
        
        #print(f"{colored('CVE','yellow')}: {cve_id}")
        if str(cve_title).strip()[0:4] != 'http': 
            print(f"{colored('TITLE','yellow')}: {cve_title}")
        print(f"{colored('CSSV2','yellow')}: {v2score}:{v2severity} | {colored('CSSV3','yellow')}: {v3score}:{v3severity} | {colored('CVE','yellow')}: {colored(cve_id,'yellow')}")
        #print(f"{colored('URL','yellow')}: {ref_links[i]}")
        print(f"{colored('SUMMARY','yellow')}: {cve_description}") 
        print(f"{colored('REFERENCE','yellow')}: {colored(cve_reflink,'blue')}") 
        #print(f"{colored('DATE','yellow')}: {publish_dates[i]}")
        #print(f"{colored('SYSTEMS','yellow')}: {compatable_systems[i]}")
        #print(f"{colored('TAGS','yellow')}: {tag}")
        print()  

        #### banner + output

    #print(vulns)    

### 
# addOns=dictionaryCpes 
# apiKey optional
# cpeMatchString optional
# cvssV2Severity=HIGH|MEDIUM|LOW
# cvssV3Severity=CRITICAL|HIGH|MEDIUM|LOW
# isExactMatch=true optional
# keyword= found in the vulnerability description or reference links. 

def main(): 
    query_packetstorm() # refactored
    
    nvd_api() # works


    # ### Packetstorm Section 
    # f"https://packetstormsecurity.com/search/?q={SEARCHTERM}"

    # ### Rapid7
    # f"https://www.rapid7.com/db/?q={SEARCHTERM}&type="
    # site:www.rapid7.com/db/modules/exploit/* "{SEARCHTERM}"

    # ### EDB Section 
    # f"https://www.exploit-db.com/search?q={SEARCHTERM}"

    # ### CVE Details Section 
    # f"https://www.cvedetails.com/google-search-results.php?q={SEARCHTERM}&sa=Search"

    # if CVE_ID: 
    #     f"https://www.exploit-db.com/search?q={SEARCHTERM}&cve={CVE_ID}"
    #     f"https://www.cvedetails.com/cve-details.php?t=1&cve_id=CVE-{CVE_ID}"


if __name__ == '__main__': 
    main() 